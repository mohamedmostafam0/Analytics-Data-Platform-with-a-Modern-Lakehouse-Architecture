services:
  connect:
    container_name: connect
    build:
      context: ./kafka-connect
    depends_on:
      - kafka
      - debezium-postgres
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect-configs
      OFFSET_STORAGE_TOPIC: connect-offsets
      STATUS_STORAGE_TOPIC: connect-status
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      PLUGIN_PATH: /kafka/connect
    networks:
      iceberg_net:


  debezium-postgres:
    container_name: debezium-postgres
    image: debezium/postgres:17
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: ${CDC_POSTGRES_USER}
      POSTGRES_PASSWORD: ${CDC_POSTGRES_PASSWORD}
      POSTGRES_DB: ${CDC_POSTGRES_DB}
    volumes:
      - ./postgres/postgres_bootstrap.sql:/docker-entrypoint-initdb.d/postgres_bootstrap.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${CDC_POSTGRES_USER} -d ${CDC_POSTGRES_DB}" ]
      interval: 1s
      start_period: 60s
    networks:
      iceberg_net:


  items-loadgen:
    build: load-generators/items-load
    container_name: items-loadgen
    init: true
    networks:
      iceberg_net:
    depends_on:
      debezium-postgres:
        condition: service_healthy
    environment:
      POSTGRES_HOST: debezium-postgres
      POSTGRES_DB: ${CDC_POSTGRES_DB}
      POSTGRES_USER: ${CDC_POSTGRES_USER}
      POSTGRES_PASSWORD: ${CDC_POSTGRES_PASSWORD}

  login-loadgen:
    build: load-generators/login-load
    container_name: login-loadgen
    init: true
    networks:
      iceberg_net:
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    environment:
      KAFKA_BROKERS: kafka:9092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081

  kafka:
    container_name: kafka
    image: apache/kafka:3.7.1
    ports:
      - "9092:9092"
    environment:
      # KRaft settings
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: controller,broker
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # General settings
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    volumes:
      - kafka_data:/tmp/kraft-combined-logs
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list" ]
      interval: 5s
      timeout: 10s
      retries: 5
    networks:
      iceberg_net:
        aliases:
          - kafka

  kafka-setup:
    image: apache/kafka:3.7.1
    container_name: kafka-setup
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      bash -c "
        /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic _schemas --partitions 1 --replication-factor 1 --config cleanup.policy=compact &&
        /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic login-events --partitions 1 --replication-factor 1 &&
        /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic login-events-enriched --partitions 1 --replication-factor 1 &&
        /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --topic login-anomalies --partitions 1 --replication-factor 1
      "
    networks:
      iceberg_net:


  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka:9092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      iceberg_net:


  loadgen:
    build: load-generators/sys-load/
    container_name: loadgen
    networks:
      iceberg_net:
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
    env_file:
      - .env

  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    networks:
      iceberg_net:
    env_file:
      - .env
    entrypoint: |
      /bin/sh -c "
      until (/usr/bin/mc alias set minio ${CATALOG_S3_ENDPOINT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb --ignore-existing minio/warehouse;
      /usr/bin/mc mb --ignore-existing minio/pageviews;
      /usr/bin/mc anonymous set public minio/warehouse;
      tail -f /dev/null
      "

  minio:
    image: minio/minio
    container_name: minio
    env_file:
      - .env
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - minio_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  opensearch:
    image: opensearchproject/opensearch:2.11.0
    container_name: opensearch
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "DISABLE_SECURITY_PLUGIN=true"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9600:9600"
    networks:
      iceberg_net:


  postgres:
    image: postgres:18
    hostname: postgres
    container_name: postgres
    networks:
      iceberg_net:
    ports:
      - 5432:5432
    env_file:
      - .env
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./postgres/postgres_bootstrap.sql:/docker-entrypoint-initdb.d/postgres_bootstrap.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 1s
      start_period: 60s

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    container_name: redpanda-console
    ports:
      - "8084:8080"
    environment:
      KAFKA_BROKERS: kafka:9092
      KAFKA_SCHEMAREGISTRY_ENABLED: "true"
      KAFKA_SCHEMAREGISTRY_URLS: http://schema-registry:8081
    networks:
      iceberg_net:


  connector-setup:
    image: debian:bookworm-slim
    container_name: connector-setup
    depends_on:
      - connect
    volumes:
      - ./kafka-connect:/kafka-connect
    working_dir: /kafka-connect
    environment:
      CONNECT_HOST: connect
      CONNECT_PORT: 8083
      CDC_POSTGRES_USER: ${CDC_POSTGRES_USER}
      CDC_POSTGRES_PASSWORD: ${CDC_POSTGRES_PASSWORD}
      CDC_POSTGRES_DB: ${CDC_POSTGRES_DB}
    entrypoint: [ "/bin/bash", "-c", "apt-get update && apt-get install -y curl gettext-base dos2unix jq && dos2unix /kafka-connect/register_connector.sh && bash /kafka-connect/register_connector.sh" ]
    networks:
      iceberg_net:


  rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    networks:
      iceberg_net:
    ports:
      - 8181:8181
    env_file:
      - .env
    environment:
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO

  spark-iceberg:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg
    build: spark/
    networks:
      iceberg_net:
    depends_on:
      - rest
      - minio
    volumes:
      - ./warehouse:/home/iceberg/warehouse
      - ./notebooks:/home/iceberg/notebooks/notebooks
      - ./spark/scripts:/home/iceberg/scripts
    env_file:
      - .env
    ports:
      - 8080:8080
      - 10000:10000
      - 10001:10001

  superset:
    build: ./superset
    container_name: superset
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      ADMIN_USERNAME: ${SUPERSET_ADMIN_USERNAME}
      ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      ADMIN_FIRST_NAME: Superset
      ADMIN_LAST_NAME: Admin
      ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL}
      DATABASE_URL: postgresql+psycopg2://${SUPERSET_DB_USER}:${SUPERSET_DB_PASSWORD}@superset-db:5432/${SUPERSET_DB}
    depends_on:
      - superset-db
    ports:
      - "8088:8088"
    volumes:
      - ./superset/superset_home:/app/superset_home
      - ./superset/init_connections.py:/app/init_connections.py
    networks:
      iceberg_net:
    command: >
      /bin/sh -c "
        superset db upgrade &&
        superset fab create-admin --username ${SUPERSET_ADMIN_USERNAME} --firstname Superset --lastname Admin --email ${SUPERSET_ADMIN_EMAIL} --password ${SUPERSET_ADMIN_PASSWORD} || true &&
        superset init &&
        python /app/init_connections.py &&
        superset run -h 0.0.0.0 -p 8088
      "

  superset-db:
    image: postgres:18
    container_name: superset-db
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${SUPERSET_DB}
      POSTGRES_USER: ${SUPERSET_DB_USER}
      POSTGRES_PASSWORD: ${SUPERSET_DB_PASSWORD}
    volumes:
      - ./superset/superset_db_data:/var/lib/postgresql/data
    networks:
      iceberg_net:


  trino:
    image: 'trinodb/trino'
    hostname: trino
    container_name: trino
    volumes:
      - ./trino/etc/catalog:/etc/trino/catalog
    ports:
      - '9090:8080'
    networks:
      iceberg_net:


  flashsale-loadgen:
    build: load-generators/flashsale-load
    container_name: flashsale-loadgen
    init: true
    depends_on:
      debezium-postgres:
        condition: service_healthy
    environment:
      POSTGRES_HOST: debezium-postgres
      POSTGRES_DB: ${CDC_POSTGRES_DB}
      POSTGRES_USER: ${CDC_POSTGRES_USER}
      POSTGRES_PASSWORD: ${CDC_POSTGRES_PASSWORD}
      USERS_SEED_COUNT: 1000
      ITEM_SEED_COUNT: 100
      PURCHASE_GEN_EVERY_MS: 500
    networks:
      iceberg_net:


  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    depends_on:
      - schema-registry
      - kafka
    volumes:
      - ./clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql
    environment:
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-mysecret}
    healthcheck:
      test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8123/ping" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
    ports:
      - "9002:9000"
      - "8123:8123"
    ulimits:
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144
    networks:
      iceberg_net:


  streamlit:
    build: streamlit
    container_name: streamlit
    init: true
    ports:
      - "8501:8501"
    depends_on:
      - clickhouse
    networks:
      iceberg_net:


  jobmanager:
    image: flink:1.17.2-scala_2.12-java11
    container_name: jobmanager
    ports:
      - 8082:8081
    command: jobmanager
    volumes:
      - ./flink/sql:/opt/flink/flink-sql
      - ./flink/lib/flink-sql-connector-kafka-1.17.2.jar:/opt/flink/lib/flink-sql-connector-kafka-1.17.2.jar
      - ./ip_geo.csv:/opt/flink/ip_geo.csv
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager        
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      iceberg_net:


  taskmanager:
    image: flink:1.17.2-scala_2.12-java11
    container_name: taskmanager
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    volumes:
      - ./flink/lib/flink-sql-connector-kafka-1.17.2.jar:/opt/flink/lib/flink-sql-connector-kafka-1.17.2.jar
      - ./ip_geo.csv:/opt/flink/ip_geo.csv
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 20
    networks:
      iceberg_net:


networks:
  iceberg_net:


volumes:
  minio_data:
  kafka_data:
