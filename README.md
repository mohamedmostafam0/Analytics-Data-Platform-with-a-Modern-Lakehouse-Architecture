# Analytics Data Platform with a Modern Lakehouse Architecture

![Project Architecture](docs/architecture.svg)

### Data Flow
![Data Flow](docs/data_flow.svg)

## ğŸš€ Overview

This project implements a **Modern Data Lakehouse** architecture, combining the best features of data lakes and data warehouses. It provides a robust, scalable, and open platform for data engineering and analytics workloads.

The platform follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) and is built on open standards, leveraging **Apache Iceberg** for table format, **Apache Spark** for compute, **Trino** for interactive queries, and **Apache Superset** for visualization.

## âœ¨ Key Features

-   **Open Table Format**: Apache Iceberg for ACID transactions, time travel, and schema evolution.
-   **Scalable Compute**: Apache Spark 3.5 for large-scale data processing (ETL).
-   **Real-time Streaming**: Kafka + Debezium for Change Data Capture (CDC) from Postgres.
-   **Search & Analytics**: OpenSearch for full-text search and log analytics.
-   **Interactive SQL**: Trino for low-latency, ad-hoc analytical queries.
-   **BI & Visualization**: Apache Superset dashboards connected via Trino.
-   **S3-Compatible Storage**: MinIO provides high-performance object storage.
-   **REST Catalog**: Centralized Iceberg metadata management.
-   **Medallion Architecture**: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (aggregated).

## ğŸ› ï¸ Tech Stack

| Component | Technology | Description |
| :--- | :--- | :--- |
| **Compute (ETL)** | [Apache Spark](https://spark.apache.org/) 3.5 | Batch data processing and transformations. |
| **Streaming** | [Apache Kafka](https://kafka.apache.org/) | Event streaming platform. |
| **Stream Processing** | [Apache Flink](https://flink.apache.org/) 1.17 | Stateful stream processing and bounded-state anomaly detection. |
| **CDC** | [Debezium](https://debezium.io/) | Change Data Capture for Postgres. |
| **Search Engine** | [OpenSearch](https://opensearch.org/) | Distributed search and analytics engine. |
| **Schema Registry** | [Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html) | Strict Avro schema enforcement and management. |
| **Query Engine** | [Trino](https://trino.io/) | Interactive SQL queries for analytics / BI. |
| **Table Format** | [Apache Iceberg](https://iceberg.apache.org/) | Open table format for huge analytic datasets. |
| **Storage** | [MinIO](https://min.io/) | S3-compatible object storage. |
| **Catalog** | Iceberg REST | Centralized metadata catalog. |
| **Visualization** | [Apache Superset](https://superset.apache.org/) | BI dashboards and data exploration. |
| **Orchestration** | [Apache Airflow](https://airflow.apache.org/) 2.8 | DAG-based workflow orchestration. |
| **OLTP Database** | [PostgreSQL](https://www.postgresql.org/) 18 | Source transactional database. |
| **Data Generator** | Custom Python | Multi-purpose synthetic data generators (E-commerce + Auth streaming). |
| **Email (Dev)** | [MailHog](https://github.com/mailhog/MailHog) | Local SMTP server for testing email alerts. |
| **Containerization** | Docker Compose | Local container orchestration. |

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ .env                            # Environment variables (single source of truth)
â”œâ”€â”€ .env.example                    # Template for new setups
â”œâ”€â”€ docker-compose.yaml             # Core services (Spark, Trino, MinIO, Superset, Postgres)
â”œâ”€â”€ airflow.yaml                    # Airflow services (webserver, scheduler, DB, MailHog)
â”œâ”€â”€ scripts/                        # Utility scripts
â”‚   â””â”€â”€ lakehouse-preparer.sh       # End-to-end pipeline orchestrator
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ airflow/                        # Airflow orchestration
â”‚   â”œâ”€â”€ dockerfile
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ user_engagement_segments_dag.py
â”‚       â””â”€â”€ sql/
â”‚           â””â”€â”€ trino.sql           # Gold-layer segmentation query
â”‚
â”œâ”€â”€ load-generators/                # Data Generators
â”‚   â”œâ”€â”€ sys-load/                   # System Load (CPU/Memory)
â”‚   â”œâ”€â”€ items-load/                 # Product Seeder
â”‚   â”œâ”€â”€ flashsale-load/             # Crash simulation (Purchases)
â”‚   â”œâ”€â”€ login-load/                 # Real-time Auth Event Simulator (Avro)
â”‚   â””â”€â”€ README.md                   # Generator Documentation
â”‚   â””â”€â”€ postgres_bootstrap.sql
â”‚
â”œâ”€â”€ kafka-connect/                  # Streaming Pipeline Configs
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ connector.json              # Debezium Source Config
â”‚   â”œâ”€â”€ opensearch-sink.json        # OpenSearch Sink Config
â”‚   â””â”€â”€ register_connector.sh
â”‚
â”œâ”€â”€ flink/                          # Flink Resources
â”‚   â”œâ”€â”€ sql/                        # Flink SQL Jobs
â”‚   â”‚   â”œâ”€â”€ create-tables.sql
â”‚   â”‚   â””â”€â”€ insert-jobs.sql
â”‚   â””â”€â”€ lib/                        # Connectors (JARs)
â”‚
â”œâ”€â”€ spark/                          # Spark image & ETL scripts
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â”œâ”€â”€ spark-defaults.conf
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ sql/                    # Iceberg DDL (per-layer)
â”‚       â”‚   â”œâ”€â”€ bronze_schema.sql
â”‚       â”‚   â”œâ”€â”€ silver_schema.sql
â”‚       â”‚   â””â”€â”€ gold_schema.sql
â”‚       â”œâ”€â”€ config.py               # Centralized configuration
â”‚       â”œâ”€â”€ etl_utils.py            # Shared utilities
â”‚       â”œâ”€â”€ minio_loader.py         # Bronze: MinIO â†’ Iceberg
â”‚       â”œâ”€â”€ postgres_loader.py      # Bronze: Postgres â†’ Iceberg
â”‚       â”œâ”€â”€ bronze_to_silver_transformer.py
â”‚       â”œâ”€â”€ silver_to_gold_transformer.py
â”‚       â””â”€â”€ tests/
â”‚
â”œâ”€â”€ superset/                       # Superset image config
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ init_connections.py         # Auto-creates Trino database connection
â”‚
â””â”€â”€ trino/                          # Trino catalog config
    â””â”€â”€ etc/catalog/
        â””â”€â”€ iceberg.properties
```

## âš¡ Getting Started

### Prerequisites

-   [Docker](https://www.docker.com/)
-   [Docker Compose](https://docs.docker.com/compose/)

### Installation

1.  **Clone the repository**:
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Configure Environment**:
    ```bash
    cp .env.example .env
    ```
    > The defaults work out-of-the-box for local development.

3.  **Start the Core Services**:
    ```bash
    docker compose up -d --build
    ```

4.  **Start Airflow** (optional):
    ```bash
    docker compose -f airflow.yaml up -d --build
    ```

5.  **Run the Data Pipeline**:
    ```bash
    # Generate synthetic data
    docker compose run loadgen

    # Run full pipeline (schemas â†’ ingest â†’ transform)
    chmod +x scripts/lakehouse-preparer.sh
    ./scripts/lakehouse-preparer.sh
    ```

## ğŸ–¥ï¸ Services

| Service | URL | Credentials |
| :--- | :--- | :--- |
| **Superset** | [http://localhost:8088](http://localhost:8088) | `admin` / `admin` |
| **Airflow** | [http://localhost:8085](http://localhost:8085) | `admin` / `admin` |
| **MailHog** | [http://localhost:8025](http://localhost:8025) | â€” |
| **Trino** | `http://localhost:9090` | â€” |
| **MinIO Console** | [http://localhost:9001](http://localhost:9001) | `minioadmin` / `minioadmin` |
| **MinIO API** | `http://localhost:9000` | â€” |
| **Iceberg REST** | `http://localhost:8181` | â€” |
| **Redpanda Console** | [http://localhost:8084](http://localhost:8084) | â€” |
| **Schema Registry** | [http://localhost:8081](http://localhost:8081) | â€” |
| **OpenSearch** | [http://localhost:9200](http://localhost:9200) | â€” |
| **Spark UI** | [http://localhost:8080](http://localhost:8080) | â€” |
| **PostgreSQL** | `localhost:5432` | See `.env` |

## ğŸ­ Data Pipeline

The pipeline follows the Medallion Architecture:

```
Sources                    Bronze              Silver                Gold
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚â”€â”€JDBCâ”€â”€â–¶ â”‚ users      â”‚â”€â”€â–¶  â”‚ users         â”‚    â”‚ top_selling_items    â”‚
â”‚ (Users,  â”‚          â”‚ items      â”‚     â”‚ items         â”‚â”€â”€â–¶ â”‚ sales_perf_24h       â”‚
â”‚  Items,  â”‚          â”‚ purchases  â”‚â”€â”€â–¶  â”‚ purchases_    â”‚    â”‚ top_converting       â”‚
â”‚  Purch.) â”‚          â”‚            â”‚     â”‚   enriched    â”‚    â”‚ pageviews_by_ch      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ user_engagement_segs â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ MinIO    â”‚â”€â”€S3â”€â”€â”€â”€â–¶ â”‚ pageviews  â”‚â”€â”€â–¶  â”‚ pageviews_    â”‚
â”‚ (JSON)   â”‚          â”‚ (+ DLQ)    â”‚     â”‚   by_items    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Note:** `user_engagement_segments` is computed by the Airflow DAG (via Trino), not by Spark.

### Real-time Streaming Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚â”€â”€CDCâ”€â–¶ Kafka Topic  â”‚â”€â”€Sinkâ”€â–¶ OpenSearch â”‚â”€â”€APIâ”€â–¶ Architecture â”‚
â”‚ (Items)  â”‚      â”‚ (Avro)       â”‚      â”‚ (Items)    â”‚      â”‚ Diagram / UI â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Flink  â”‚â”€â”€SQLâ”€â–¶ Login Anomaliesâ”‚
                          â”‚ (SQL)   â”‚      â”‚ (Kafka JSON)  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–²
                               â”‚ Avro Login Events
                               â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Login Loadgenâ”‚â”€â”€Avroâ”€â–¶ Schema      â”‚
                        â”‚ (Simulator)  â”‚      â”‚  Registry    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Running Individual Steps

```bash
# 1. Generate data
docker-compose run loadgen

# 2. Create schemas
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/bronze_schema.sql
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/silver_schema.sql
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/gold_schema.sql

# 3. Ingest to Bronze
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/minio_loader.py
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/postgres_loader.py

# 4. Transform Bronze â†’ Silver
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/bronze_to_silver_transformer.py

# 5. Transform Silver â†’ Gold
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/silver_to_gold_transformer.py
```

### ğŸ§ª Running Tests

```bash
docker exec spark-iceberg pytest /home/iceberg/scripts/tests/
```

## ğŸ“¸ Screenshots

### Superset Dashboard
![Superset Dashboard](docs/screenshots/superset-dashboard.png)

### MinIO Console
![MinIO Console](docs/screenshots/minio-console.png)

### Airflow DAG Graph
![Airflow DAG](docs/screenshots/airflow-dag-graph.png)

### Streamlit Real-Time Dashboard
<!-- TODO: Add screenshot of real-time ClickHouse metrics at http://localhost:8501 -->
![Streamlit Dashboard](docs/screenshots/streamlit-dashboard.png)

### Kafka Topics in Redpanda Console 
<!-- TODO: Add screenshot of Avro schemas or Topic UI at http://localhost:8084 -->
![Redpanda Console](docs/screenshots/redpanda-console.png)

### Flink Pipeline Topology
![Flink Topology - Login Events Enriched](docs/screenshots/login_events_enriched_flink.png)

![Flink Topology - Login Anomalies](docs/screenshots/login_anomalies_flink.png)

### Spark UI
<!-- TODO: Add screenshot of Spark UI showing completed ETL jobs -->
<!-- ![Spark UI](docs/screenshots/spark-ui.png) -->

---

## ğŸ” Querying Data

### Via Trino (CLI)
```bash
docker exec trino trino --execute "SELECT * FROM iceberg.gold.top_selling_items ORDER BY total_revenue DESC LIMIT 10"
```

### Via Superset
1. Open [http://localhost:8088](http://localhost:8088) and login with `admin` / `admin`.
2. The Trino database connection (`trino://trino@trino:8080/iceberg`) is auto-created at startup.
3. Create charts and dashboards from the `gold` schema tables.

### Via Airflow
1. Open [http://localhost:8085](http://localhost:8085) and login with `admin` / `admin`.
2. Trino and MinIO connections are auto-created via `AIRFLOW_CONN_` env vars.
3. Enable the `user_engagement_segments_dag` to run the daily segmentation pipeline.
4. Check email alerts in [MailHog](http://localhost:8025).
