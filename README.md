# Analytics Data Platform with a Modern Lakehouse Architecture

![Lakehouse Architecture](lakehouse%20architecture.png)

## ğŸš€ Overview

This project implements a **Modern Data Lakehouse** architecture, combining the best features of data lakes and data warehouses. It provides a robust, scalable, and open platform for data engineering and analytics workloads.

The platform follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) and is built on open standards, leveraging **Apache Iceberg** for table format, **Apache Spark** for compute, **Trino** for interactive queries, and **Apache Superset** for visualization.

## âœ¨ Key Features

-   **Open Table Format**: Apache Iceberg for ACID transactions, time travel, and schema evolution.
-   **Scalable Compute**: Apache Spark 3.5 for large-scale data processing (ETL).
-   **Interactive SQL**: Trino for low-latency, ad-hoc analytical queries.
-   **BI & Visualization**: Apache Superset dashboards connected via Trino.
-   **S3-Compatible Storage**: MinIO provides high-performance object storage.
-   **REST Catalog**: Centralized Iceberg metadata management.
-   **Medallion Architecture**: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (aggregated).

## ğŸ› ï¸ Tech Stack

| Component | Technology | Description |
| :--- | :--- | :--- |
| **Compute (ETL)** | [Apache Spark](https://spark.apache.org/) 3.5 | Batch data processing and transformations. |
| **Query Engine** | [Trino](https://trino.io/) | Interactive SQL queries for analytics / BI. |
| **Table Format** | [Apache Iceberg](https://iceberg.apache.org/) | Open table format for huge analytic datasets. |
| **Storage** | [MinIO](https://min.io/) | S3-compatible object storage. |
| **Catalog** | Iceberg REST | Centralized metadata catalog. |
| **Visualization** | [Apache Superset](https://superset.apache.org/) | BI dashboards and data exploration. |
| **OLTP Database** | [PostgreSQL](https://www.postgresql.org/) 18 | Source transactional database. |
| **Data Generator** | Custom Python | Synthetic e-commerce data. |
| **Orchestration** | Docker Compose | Local container orchestration. |

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ .env                            # Environment variables (single source of truth)
â”œâ”€â”€ .env.example                    # Template for new setups
â”œâ”€â”€ docker-compose.yaml             # All service definitions
â”œâ”€â”€ lakehouse-preparer.sh           # End-to-end pipeline orchestration script
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ loadgen/                        # Synthetic data generator
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ generate_load.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ postgres/                       # Postgres initialization
â”‚   â””â”€â”€ postgres_bootstrap.sql
â”‚
â”œâ”€â”€ spark/                          # Spark image & ETL scripts
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â”œâ”€â”€ spark-defaults.conf
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ sql/                    # Iceberg DDL (per-layer)
â”‚       â”‚   â”œâ”€â”€ bronze_schema.sql
â”‚       â”‚   â”œâ”€â”€ silver_schema.sql
â”‚       â”‚   â””â”€â”€ gold_schema.sql
â”‚       â”œâ”€â”€ config.py               # Centralized configuration
â”‚       â”œâ”€â”€ etl_utils.py            # Shared utilities
â”‚       â”œâ”€â”€ minio_loader.py         # Bronze: MinIO â†’ Iceberg
â”‚       â”œâ”€â”€ postgres_loader.py      # Bronze: Postgres â†’ Iceberg
â”‚       â”œâ”€â”€ bronze_to_silver_transformer.py
â”‚       â”œâ”€â”€ silver_to_gold_transformer.py
â”‚       â””â”€â”€ tests/
â”‚
â”œâ”€â”€ superset/                       # Superset image config
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ trino/                          # Trino catalog config
    â””â”€â”€ etc/catalog/
        â””â”€â”€ iceberg.properties
```

## âš¡ Getting Started

### Prerequisites

-   [Docker](https://www.docker.com/)
-   [Docker Compose](https://docs.docker.com/compose/)

### Installation

1.  **Clone the repository**:
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Configure Environment**:
    ```bash
    cp .env.example .env
    ```
    > The defaults work out-of-the-box for local development.

3.  **Start the Services**:
    ```bash
    docker-compose up -d --build
    ```

4.  **Run the Data Pipeline**:
    ```bash
    # Generate synthetic data
    docker-compose run loadgen

    # Run full pipeline (schemas â†’ ingest â†’ transform)
    chmod +x lakehouse-preparer.sh
    ./lakehouse-preparer.sh
    ```

## ğŸ–¥ï¸ Services

| Service | URL | Credentials |
| :--- | :--- | :--- |
| **Superset** | [http://localhost:8088](http://localhost:8088) | `admin` / `admin` |
| **Trino** | `http://localhost:9090` | â€” |
| **MinIO Console** | [http://localhost:9001](http://localhost:9001) | `minioadmin` / `minioadmin` |
| **MinIO API** | `http://localhost:9000` | â€” |
| **Iceberg REST** | `http://localhost:8181` | â€” |
| **Spark UI** | [http://localhost:8080](http://localhost:8080) | â€” |
| **PostgreSQL** | `localhost:5432` | `admin` / `password` |

## ğŸ­ Data Pipeline

The pipeline follows the Medallion Architecture:

```
Sources                    Bronze              Silver                Gold
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚â”€â”€JDBCâ”€â”€â–¶ â”‚ users      â”‚â”€â”€â–¶  â”‚ users         â”‚    â”‚ top_selling_items â”‚
â”‚ (Users,  â”‚          â”‚ items      â”‚     â”‚ items         â”‚â”€â”€â–¶ â”‚ sales_perf_24h   â”‚
â”‚  Items,  â”‚          â”‚ purchases  â”‚â”€â”€â–¶  â”‚ purchases_    â”‚    â”‚ top_converting   â”‚
â”‚  Purch.) â”‚          â”‚            â”‚     â”‚   enriched    â”‚    â”‚ pageviews_by_ch  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO    â”‚â”€â”€S3â”€â”€â”€â”€â–¶ â”‚ pageviews  â”‚â”€â”€â–¶  â”‚ pageviews_    â”‚
â”‚ (JSON)   â”‚          â”‚ (+ DLQ)    â”‚     â”‚   by_items    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Running Individual Steps

```bash
# 1. Generate data
docker-compose run loadgen

# 2. Create schemas
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/bronze_schema.sql
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/silver_schema.sql
docker-compose exec spark-iceberg /opt/spark/bin/spark-sql -f /home/iceberg/scripts/sql/gold_schema.sql

# 3. Ingest to Bronze
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/minio_loader.py
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/postgres_loader.py

# 4. Transform Bronze â†’ Silver
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/bronze_to_silver_transformer.py

# 5. Transform Silver â†’ Gold
docker-compose exec spark-iceberg /opt/spark/bin/spark-submit /home/iceberg/scripts/silver_to_gold_transformer.py
```

### ğŸ§ª Running Tests

```bash
docker exec spark-iceberg pytest /home/iceberg/scripts/tests/
```

## ğŸ“¸ Screenshots

### Superset Dashboard
<!-- TODO: Add screenshot of Superset dashboard with Gold layer charts -->
![Superset Dashboard](screenshots/superset-dashboard.png)

### MinIO Console
<!-- TODO: Add screenshot of MinIO console showing warehouse bucket -->
![MinIO Console](screenshots/minio-console.png)

### Trino Query Results
<!-- TODO: Add screenshot of Trino querying gold tables -->
![Trino Query](screenshots/trino-query.png)

### Spark UI
<!-- TODO: Add screenshot of Spark UI showing completed ETL jobs -->
![Spark UI](screenshots/spark-ui.png)

---

## ğŸ” Querying Data

### Via Trino (CLI)
```bash
docker exec trino trino --execute "SELECT * FROM iceberg.gold.top_selling_items ORDER BY total_revenue DESC LIMIT 10"
```

### Via Superset
1. Open [http://localhost:8088](http://localhost:8088) and login with `admin` / `admin`.
2. Add a Trino database connection: `trino://trino@trino:8080/iceberg`.
3. Create charts and dashboards from the `gold` schema tables.
